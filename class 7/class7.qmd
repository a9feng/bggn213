---
title: "Class 7: Machine learning 1"
author: "Anqi Feng (PID:A16243334)"
format: pdf
---

Before we get into clustering methods, let's make some sample datas to cluster where we know what the answer should be.

To help with this, I will use the `rnorm()` function

```{r}
hist(c(rnorm(150000, mean=3),rnorm(150000, mean=-3)))
```

```{r}
n=30
x <- c(rnorm(n,mean=3), rnorm(n, mean = -3))
x
y <-rev(x)
z<- cbind(x,y)
plot(z)
```
##K-means clustering
The function in base R, the main function for kmeans clustering is called `kmeans()`.
kmeans(x, centers, iter.max = 10, nstart = 1, algorithm = c(...), trace = FALSE)
centers: num of clusters or "k"
```{r}
km<- kmeans(z,centers = 2)
```
```{r}
km$centers
```

Q: print out the cluster membership vector (ie our main answer)
```{r}
km$cluster
```
```{r}
plot(z, col = c("red", "blue"))
```
Plot with clustering result and add cluster centers:
```{r}
plot(z,col = km$cluster)
points(km$centers, col = "blue", pch=15, cex=2)
```
Q. Can you cluster our data in 'z' into four clusters please?
```{r}
km4 <- kmeans(z,centers = 4)
plot(z, col=km4$cluster)
points(km4$centers, col="blue", pch=15, cex=2)
```
##Hierarchical clustering
The main function to do hierarchical clustering in base R is called `hclust()`
unlike `kmeans()`I cannot just pass in my data as input I first need a distance matrix from my data.
```{r}
d <- dist(z)
hc <- hclust(d)
hc
```
There is a specific hclust plot method ...
```{r}
plot(hc)
abline(h=10, col = "red")
```
To get my main clustering result(i.e. the membership vector) I can "cut" my tree at a wanted height, to do this I will use the `cutree()`
```{r}
grps<- cutree(hc, h=10)
grps
```
##PCA
is a well established multivariable statisticall tool to decrease dimentionality
##UK food problem example exercise
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
dim(x)
```
```{r}
##Preview the first 6 rows
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```
```{r}
dim(x)
```
```{r}
x <- read.csv(url, row.names=1)
head(x)
```
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
pairs(x, col=rainbow(10), pch=16)
```
##PCA to the rescue
the main function of PCA is called `prcomp()`
Note that I need to take the transpose this particular data as that is what the   `prcomp()` help page was asking for
```{r}
pca <- prcomp(t(x))
summary(pca)
```
Let's see what is inside our result object pca that we just calculated:
```{r}
attributes(pca)
```
To make a results figure of the "PC plot" or "score plot" or "coordination plot" or "PC1 vs PC2 plot".
```{r}
plot(pca$x[,1], pca$x[,2], col=c("black", "red", "blue", "darkgreen"),pch = 16, xlab="PC1(67.4%)", ylab="PC2(29%)")
abline(h=0, col="gray")
abline(v=0, col="gray")
```
##Variable Loadings
Can give us insights on original variables(in tihs case the foods) contribute to >90% of the variants
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
Fresh potatos and soft drinks are more in scotland, and less fresh fruit and alcoholic drinks compared to others
